{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RNA 3D Structure Prediction - Inference\n",
        "\n",
        "Generate submission for Stanford RNA 3D Folding competition.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "print(f\"[{time.strftime('%H:%M:%S')}] Starting inference...\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "CONFIG = {\n",
        "    'data_dir': '../input/stanford-rna-3d-folding-2',\n",
        "    'model_path': '../input/stanford-rna-3d-folding-starter/model.pth',\n",
        "    'max_len': 384,\n",
        "    'vocab_size': 5,\n",
        "    'embed_dim': 256,\n",
        "    'nhead': 8,\n",
        "    'num_layers': 6,\n",
        "    'num_predictions': 5,\n",
        "    'dropout': 0.0,\n",
        "}\n",
        "\n",
        "# Training normalization stats from logs\n",
        "TRAIN_MEAN = np.array([158.24141, 156.78705, 154.46898])\n",
        "TRAIN_STD = np.array([131.6201, 132.98083, 127.77188])\n",
        "\n",
        "print(\"Config loaded\")\n",
        "print(f\"Normalization - Mean: {TRAIN_MEAN}, Std: {TRAIN_STD}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class RNAStructurePredictor(nn.Module):\n",
        "    def __init__(self, vocab_size=5, embed_dim=256, nhead=8, num_layers=6,\n",
        "                 num_predictions=5, dropout=0.0, max_len=512):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_predictions = num_predictions\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=4)\n",
        "        self.pos_encoder = PositionalEncoding(embed_dim, max_len=max_len)\n",
        "        self.embed_dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim, nhead=nhead, dim_feedforward=embed_dim*4,\n",
        "            dropout=dropout, batch_first=True, norm_first=True, activation='gelu'\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        \n",
        "        self.prediction_heads = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(embed_dim, embed_dim), nn.GELU(), nn.Dropout(dropout),\n",
        "                nn.Linear(embed_dim, embed_dim // 2), nn.GELU(),\n",
        "                nn.Linear(embed_dim // 2, 3)\n",
        "            ) for _ in range(num_predictions)\n",
        "        ])\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        x_embed = self.embedding(x)\n",
        "        x_embed = self.pos_encoder(x_embed)\n",
        "        x_embed = self.embed_dropout(x_embed)\n",
        "        padding_mask = ~mask if mask is not None else None\n",
        "        encoded = self.transformer(x_embed, src_key_padding_mask=padding_mask)\n",
        "        predictions = [head(encoded) for head in self.prediction_heads]\n",
        "        return torch.stack(predictions, dim=3)\n",
        "\n",
        "print(\"Model class defined\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"[{time.strftime('%H:%M:%S')}] Loading model...\")\n",
        "\n",
        "model = RNAStructurePredictor(\n",
        "    vocab_size=CONFIG['vocab_size'],\n",
        "    embed_dim=CONFIG['embed_dim'],\n",
        "    nhead=CONFIG['nhead'],\n",
        "    num_layers=CONFIG['num_layers'],\n",
        "    num_predictions=CONFIG['num_predictions'],\n",
        "    dropout=CONFIG['dropout'],\n",
        "    max_len=CONFIG['max_len']\n",
        ").to(device)\n",
        "\n",
        "# Load checkpoint - weights_only=False for PyTorch 2.6+\n",
        "print(f\"Loading from: {CONFIG['model_path']}\")\n",
        "print(f\"Path exists: {os.path.exists(CONFIG['model_path'])}\")\n",
        "\n",
        "# List available files in input directory\n",
        "import glob\n",
        "print(f\"Available files: {glob.glob('../input/stanford-rna-3d-folding-starter/*')}\")\n",
        "\n",
        "ckpt = torch.load(CONFIG['model_path'], map_location=device, weights_only=False)\n",
        "print(f\"Checkpoint type: {type(ckpt)}\")\n",
        "print(f\"Checkpoint keys: {ckpt.keys() if isinstance(ckpt, dict) else 'not a dict'}\")\n",
        "\n",
        "if isinstance(ckpt, dict) and 'model_state_dict' in ckpt:\n",
        "    model.load_state_dict(ckpt['model_state_dict'])\n",
        "    # Update normalization from checkpoint if available\n",
        "    if 'coord_mean' in ckpt:\n",
        "        TRAIN_MEAN = ckpt['coord_mean']\n",
        "        TRAIN_STD = ckpt['coord_std']\n",
        "        print(f\"Using checkpoint normalization - Mean: {TRAIN_MEAN}, Std: {TRAIN_STD}\")\n",
        "elif isinstance(ckpt, dict):\n",
        "    # Maybe it's just the state dict directly\n",
        "    model.load_state_dict(ckpt)\n",
        "else:\n",
        "    raise ValueError(f\"Unknown checkpoint format: {type(ckpt)}\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print(f\"Loaded model from epoch {ckpt.get('epoch', 'unknown') if isinstance(ckpt, dict) else 'N/A'}\")\n",
        "print(f\"[{time.strftime('%H:%M:%S')}] Model loaded!\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"[{time.strftime('%H:%M:%S')}] Loading test data...\")\n",
        "\n",
        "test_seq = pd.read_csv(os.path.join(CONFIG['data_dir'], 'test_sequences.csv'))\n",
        "print(f\"Test sequences: {len(test_seq)}\")\n",
        "print(f\"Columns: {test_seq.columns.tolist()}\")\n",
        "print(test_seq.head())\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def predict_structure(sequence, model, device, max_len=384):\n",
        "    base2int = {'A': 0, 'C': 1, 'G': 2, 'U': 3, 'N': 4, 'T': 3}\n",
        "    \n",
        "    seq_ids = [base2int.get(c.upper(), 4) for c in sequence]\n",
        "    orig_len = len(seq_ids)\n",
        "    \n",
        "    if len(seq_ids) > max_len:\n",
        "        seq_ids = seq_ids[:max_len]\n",
        "        orig_len = max_len\n",
        "    else:\n",
        "        seq_ids = seq_ids + [4] * (max_len - len(seq_ids))\n",
        "    \n",
        "    mask = torch.zeros(max_len, dtype=torch.bool)\n",
        "    mask[:orig_len] = True\n",
        "    \n",
        "    seq_tensor = torch.tensor([seq_ids], dtype=torch.long).to(device)\n",
        "    mask_tensor = mask.unsqueeze(0).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        predictions = model(seq_tensor, mask_tensor)\n",
        "    \n",
        "    preds = predictions[0, :orig_len].cpu().numpy()\n",
        "    \n",
        "    # Denormalize\n",
        "    for i in range(5):\n",
        "        preds[:, :, i] = preds[:, :, i] * TRAIN_STD + TRAIN_MEAN\n",
        "    \n",
        "    return preds, orig_len\n",
        "\n",
        "print(\"Prediction function ready\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"[{time.strftime('%H:%M:%S')}] Generating predictions...\")\n",
        "\n",
        "results = []\n",
        "total = len(test_seq)\n",
        "\n",
        "for idx, row in test_seq.iterrows():\n",
        "    target_id = row['target_id']\n",
        "    sequence = row['sequence']\n",
        "    \n",
        "    if idx % 100 == 0:\n",
        "        print(f\"[{time.strftime('%H:%M:%S')}] Progress: {idx}/{total}\")\n",
        "    \n",
        "    preds, seq_len = predict_structure(sequence, model, device, CONFIG['max_len'])\n",
        "    \n",
        "    for resid in range(seq_len):\n",
        "        row_id = f\"{target_id}_{resid+1}\"\n",
        "        result = {'ID': row_id}\n",
        "        \n",
        "        for pred_idx in range(5):\n",
        "            result[f'x_{pred_idx+1}'] = float(preds[resid, 0, pred_idx])\n",
        "            result[f'y_{pred_idx+1}'] = float(preds[resid, 1, pred_idx])\n",
        "            result[f'z_{pred_idx+1}'] = float(preds[resid, 2, pred_idx])\n",
        "        \n",
        "        results.append(result)\n",
        "\n",
        "print(f\"[{time.strftime('%H:%M:%S')}] Generated {len(results)} rows\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"[{time.strftime('%H:%M:%S')}] Creating submission...\")\n",
        "\n",
        "submission = pd.DataFrame(results)\n",
        "print(f\"Shape: {submission.shape}\")\n",
        "print(f\"Columns: {submission.columns.tolist()}\")\n",
        "print(submission.head())\n",
        "\n",
        "# Check for issues\n",
        "nan_count = submission.isna().sum().sum()\n",
        "print(f\"NaN values: {nan_count}\")\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(f\"[{time.strftime('%H:%M:%S')}] Saved submission.csv\")\n",
        "print(f\"File size: {os.path.getsize('submission.csv') / 1e6:.2f} MB\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"[{time.strftime('%H:%M:%S')}] Validating...\")\n",
        "\n",
        "sample_sub = pd.read_csv(os.path.join(CONFIG['data_dir'], 'sample_submission.csv'))\n",
        "print(f\"Sample shape: {sample_sub.shape}\")\n",
        "print(f\"Our shape: {submission.shape}\")\n",
        "print(f\"Columns match: {list(submission.columns) == list(sample_sub.columns)}\")\n",
        "\n",
        "print(f\"\\n[{time.strftime('%H:%M:%S')}] Done!\")\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}
{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stanford RNA 3D Folding - Complete Pipeline\n",
        "\n",
        "Train model and generate submission in one notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import time\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\")\n",
        "\n",
        "log(\"Starting...\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "log(f\"Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    log(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Seed\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "CONFIG = {\n",
        "    'data_dir': '../input/stanford-rna-3d-folding-2',\n",
        "    'max_len': 384,\n",
        "    'batch_size': 16,\n",
        "    'epochs': 15,\n",
        "    'lr': 1e-3,\n",
        "    'min_lr': 1e-5,\n",
        "    'weight_decay': 0.01,\n",
        "    'warmup_epochs': 2,\n",
        "    'embed_dim': 256,\n",
        "    'nhead': 8,\n",
        "    'num_layers': 6,\n",
        "    'num_predictions': 5,\n",
        "    'dropout': 0.1,\n",
        "}\n",
        "log(\"Config ready\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log(\"Loading data...\")\n",
        "train_seq = pd.read_csv(os.path.join(CONFIG['data_dir'], 'train_sequences.csv'))\n",
        "test_seq = pd.read_csv(os.path.join(CONFIG['data_dir'], 'test_sequences.csv'))\n",
        "train_labels = pd.read_csv(os.path.join(CONFIG['data_dir'], 'train_labels.csv'))\n",
        "\n",
        "log(f\"Train: {len(train_seq)}, Test: {len(test_seq)}\")\n",
        "log(f\"Labels: {len(train_labels)}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log(\"Preprocessing...\")\n",
        "\n",
        "# Build coordinates dictionary\n",
        "train_labels['target_id'] = train_labels['ID'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n",
        "coords_dict = {}\n",
        "coord_values = []\n",
        "\n",
        "for target_id, group in train_labels.groupby('target_id'):\n",
        "    group = group.sort_values('resid')\n",
        "    x = group['x_1'].values\n",
        "    y = group['y_1'].values  \n",
        "    z = group['z_1'].values\n",
        "    coords = np.stack([x, y, z], axis=1).astype(np.float32)\n",
        "    # Replace NaN with 0\n",
        "    coords = np.nan_to_num(coords, nan=0.0)\n",
        "    coords_dict[target_id] = coords\n",
        "    coord_values.append(coords)\n",
        "\n",
        "# Global normalization stats\n",
        "all_coords = np.concatenate(coord_values, axis=0)\n",
        "COORD_MEAN = np.nanmean(all_coords, axis=0)\n",
        "COORD_STD = np.nanstd(all_coords, axis=0) + 1e-6\n",
        "\n",
        "log(f\"Coords: {len(coords_dict)} targets\")\n",
        "log(f\"Mean: {COORD_MEAN}, Std: {COORD_STD}\")\n",
        "\n",
        "# Normalize\n",
        "for k in coords_dict:\n",
        "    coords_dict[k] = (coords_dict[k] - COORD_MEAN) / COORD_STD\n",
        "\n",
        "del train_labels, coord_values, all_coords\n",
        "gc.collect()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class RNADataset(Dataset):\n",
        "    def __init__(self, seq_df, coords_dict=None, max_len=384, is_test=False):\n",
        "        self.seq_df = seq_df.reset_index(drop=True)\n",
        "        self.coords_dict = coords_dict\n",
        "        self.max_len = max_len\n",
        "        self.is_test = is_test\n",
        "        self.base2int = {'A': 0, 'C': 1, 'G': 2, 'U': 3, 'N': 4, 'T': 3}\n",
        "        \n",
        "        if coords_dict and not is_test:\n",
        "            valid = set(coords_dict.keys())\n",
        "            self.seq_df = self.seq_df[self.seq_df['target_id'].isin(valid)].reset_index(drop=True)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.seq_df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.seq_df.iloc[idx]\n",
        "        seq = row['sequence']\n",
        "        \n",
        "        seq_ids = [self.base2int.get(c.upper(), 4) for c in seq]\n",
        "        orig_len = min(len(seq_ids), self.max_len)\n",
        "        \n",
        "        # Pad sequence\n",
        "        if len(seq_ids) > self.max_len:\n",
        "            seq_ids = seq_ids[:self.max_len]\n",
        "        else:\n",
        "            seq_ids = seq_ids + [4] * (self.max_len - len(seq_ids))\n",
        "        \n",
        "        mask = torch.zeros(self.max_len, dtype=torch.bool)\n",
        "        mask[:orig_len] = True\n",
        "        \n",
        "        if self.is_test:\n",
        "            return torch.tensor(seq_ids, dtype=torch.long), mask, orig_len, row['target_id']\n",
        "        \n",
        "        # Training - get coords\n",
        "        coords = self.coords_dict[row['target_id']][:self.max_len].copy()\n",
        "        if len(coords) < self.max_len:\n",
        "            coords = np.pad(coords, ((0, self.max_len - len(coords)), (0, 0)))\n",
        "        \n",
        "        return (torch.tensor(seq_ids, dtype=torch.long),\n",
        "                torch.tensor(coords, dtype=torch.float32),\n",
        "                mask, orig_len)\n",
        "\n",
        "train_dataset = RNADataset(train_seq, coords_dict, CONFIG['max_len'])\n",
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], \n",
        "                          shuffle=True, num_workers=0, drop_last=True)\n",
        "\n",
        "log(f\"Train batches: {len(train_loader)}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class RNAModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        d = cfg['embed_dim']\n",
        "        self.embedding = nn.Embedding(5, d, padding_idx=4)\n",
        "        self.pos_enc = PositionalEncoding(d, cfg['max_len'])\n",
        "        self.dropout = nn.Dropout(cfg['dropout'])\n",
        "        \n",
        "        layer = nn.TransformerEncoderLayer(d_model=d, nhead=cfg['nhead'],\n",
        "            dim_feedforward=d*4, dropout=cfg['dropout'], batch_first=True,\n",
        "            norm_first=True, activation='gelu')\n",
        "        self.transformer = nn.TransformerEncoder(layer, cfg['num_layers'])\n",
        "        \n",
        "        self.heads = nn.ModuleList([\n",
        "            nn.Sequential(nn.Linear(d, d), nn.GELU(), nn.Dropout(cfg['dropout']),\n",
        "                         nn.Linear(d, d//2), nn.GELU(), nn.Linear(d//2, 3))\n",
        "            for _ in range(cfg['num_predictions'])\n",
        "        ])\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_enc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.transformer(x, src_key_padding_mask=~mask if mask is not None else None)\n",
        "        return torch.stack([h(x) for h in self.heads], dim=3)\n",
        "\n",
        "model = RNAModel(CONFIG).to(device)\n",
        "log(f\"Model params: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train_model(model, loader, epochs):\n",
        "    criterion = nn.MSELoss(reduction='none')\n",
        "    optimizer = AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs-CONFIG['warmup_epochs'], eta_min=CONFIG['min_lr'])\n",
        "    \n",
        "    log(\"Starting training...\")\n",
        "    best_loss = float('inf')\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        \n",
        "        # Warmup\n",
        "        if epoch < CONFIG['warmup_epochs']:\n",
        "            lr = CONFIG['lr'] * (epoch + 1) / CONFIG['warmup_epochs']\n",
        "            for g in optimizer.param_groups: g['lr'] = lr\n",
        "        \n",
        "        for i, batch in enumerate(loader):\n",
        "            seq, coords, mask, _ = batch\n",
        "            seq, coords, mask = seq.to(device), coords.to(device), mask.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            preds = model(seq, mask)  # (B, L, 3, 5)\n",
        "            \n",
        "            # Expand targets for 5 predictions\n",
        "            targets = coords.unsqueeze(3).expand(-1, -1, -1, 5)\n",
        "            \n",
        "            # Masked loss\n",
        "            loss = criterion(preds, targets)\n",
        "            loss = loss.sum(dim=2)  # Sum xyz\n",
        "            loss = (loss * mask.unsqueeze(2).float()).sum() / (mask.sum() * 5 + 1e-6)\n",
        "            \n",
        "            if torch.isnan(loss) or torch.isinf(loss):\n",
        "                continue\n",
        "                \n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            if i % 50 == 0:\n",
        "                log(f\"E{epoch+1} B{i}/{len(loader)}: {loss.item():.4f}\")\n",
        "        \n",
        "        if epoch >= CONFIG['warmup_epochs']:\n",
        "            scheduler.step()\n",
        "        \n",
        "        avg_loss = total_loss / len(loader)\n",
        "        log(f\"Epoch {epoch+1}: loss={avg_loss:.4f}, lr={optimizer.param_groups[0]['lr']:.2e}\")\n",
        "        \n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save({'model': model.state_dict(), 'mean': COORD_MEAN, 'std': COORD_STD}, 'model.pth')\n",
        "            log(\"Saved best model!\")\n",
        "    \n",
        "    return best_loss\n",
        "\n",
        "best = train_model(model, train_loader, CONFIG['epochs'])\n",
        "log(f\"Training done! Best loss: {best:.4f}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log(\"Loading best model for inference...\")\n",
        "ckpt = torch.load('model.pth', map_location=device, weights_only=False)\n",
        "model.load_state_dict(ckpt['model'])\n",
        "COORD_MEAN = ckpt['mean']\n",
        "COORD_STD = ckpt['std']\n",
        "model.eval()\n",
        "\n",
        "log(\"Generating predictions...\")\n",
        "test_dataset = RNADataset(test_seq, None, CONFIG['max_len'], is_test=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "results = []\n",
        "with torch.no_grad():\n",
        "    for i, (seq, mask, orig_len, target_id) in enumerate(test_loader):\n",
        "        if i % 200 == 0:\n",
        "            log(f\"Progress: {i}/{len(test_loader)}\")\n",
        "        \n",
        "        seq, mask = seq.to(device), mask.to(device)\n",
        "        preds = model(seq, mask)  # (1, L, 3, 5)\n",
        "        \n",
        "        # Denormalize\n",
        "        preds = preds[0, :orig_len[0]].cpu().numpy()  # (L, 3, 5)\n",
        "        preds = preds * COORD_STD.reshape(1, 3, 1) + COORD_MEAN.reshape(1, 3, 1)\n",
        "        \n",
        "        for resid in range(orig_len[0]):\n",
        "            row = {'ID': f\"{target_id[0]}_{resid+1}\"}\n",
        "            for p in range(5):\n",
        "                row[f'x_{p+1}'] = float(preds[resid, 0, p])\n",
        "                row[f'y_{p+1}'] = float(preds[resid, 1, p])\n",
        "                row[f'z_{p+1}'] = float(preds[resid, 2, p])\n",
        "            results.append(row)\n",
        "\n",
        "log(f\"Generated {len(results)} predictions\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "submission = pd.DataFrame(results)\n",
        "log(f\"Submission shape: {submission.shape}\")\n",
        "print(submission.head())\n",
        "\n",
        "# Check NaN\n",
        "nan_count = submission.isna().sum().sum()\n",
        "log(f\"NaN values: {nan_count}\")\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "log(f\"Saved submission.csv ({os.path.getsize('submission.csv')/1e6:.2f} MB)\")\n",
        "\n",
        "# Verify format\n",
        "sample = pd.read_csv(os.path.join(CONFIG['data_dir'], 'sample_submission.csv'))\n",
        "log(f\"Sample shape: {sample.shape}, Our shape: {submission.shape}\")\n",
        "log(f\"Columns match: {list(submission.columns) == list(sample.columns)}\")\n",
        "\n",
        "log(\"DONE!\")\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}
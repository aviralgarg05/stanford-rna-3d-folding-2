{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf6ece97",
   "metadata": {},
   "source": [
    "# Stanford RNA 3D Folding - Baseline Model v3\n",
    "\n",
    "## 1. Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184cff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "CONFIG = {\n",
    "    'data_dir': '../input/stanford-rna-3d-folding-2',\n",
    "    'max_len': 256,\n",
    "    'batch_size': 16,\n",
    "    'epochs': 10,\n",
    "    'lr': 1e-3,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "}\n",
    "print(f\"Running on {CONFIG['device']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ad239f",
   "metadata": {},
   "source": [
    "## 2. Dataset Implementation\n",
    "\n",
    "We need to merge `train_sequences.csv` (inputs) and `train_labels.csv` (targets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b076cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNADataset(Dataset):\n",
    "    def __init__(self, data_path, max_len=256, mode='train'):\n",
    "        self.mode = mode\n",
    "        self.max_len = max_len\n",
    "        self.base2int = {c: i for i, c in enumerate('ACGU')}\n",
    "        \n",
    "        # Load Data\n",
    "        print(\"Loading dataframes...\")\n",
    "        self.seq_df = pd.read_csv(os.path.join(data_path, 'train_sequences.csv'))\n",
    "        # self.seq_df columns: ['target_id', 'sequence', ...]\n",
    "        \n",
    "        if mode == 'train':\n",
    "            lbl_df = pd.read_csv(os.path.join(data_path, 'train_labels.csv'))\n",
    "            # lbl_df columns: ['ID', 'resname', 'resid', 'x_1', 'y_1', 'z_1', ...]\n",
    "            # ID format seems to be {target_id}_{resid}\n",
    "            \n",
    "            # Add target_id column to labels for merging\n",
    "            # We assume the part before the last underscore is target_id (or similar logic)\n",
    "            # Let's inspect ID format from logs: '157D_1'\n",
    "            lbl_df['target_id'] = lbl_df['ID'].apply(lambda x: x.rsplit('_', 1)[0])\n",
    "            \n",
    "            # Group coordinates by target_id\n",
    "            # We want a list of coords for each target\n",
    "            print(\"grouping labels...\")\n",
    "            self.coords_map = lbl_df.groupby('target_id')[['x_1', 'y_1', 'z_1']].apply(lambda x: x.values).to_dict()\n",
    "            \n",
    "            # Filter sequences that have labels\n",
    "            self.seq_df = self.seq_df[self.seq_df['target_id'].isin(self.coords_map)]\n",
    "            self.seq_df = self.seq_df.reset_index(drop=True)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.seq_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.seq_df.iloc[idx]\n",
    "        target_id = row['target_id']\n",
    "        seq_str = row['sequence']\n",
    "        \n",
    "        # Convert Sequence to Ints\n",
    "        seq_ids = [self.base2int.get(c, 4) for c in seq_str] # 4 for unknown\n",
    "        \n",
    "        # Truncate\n",
    "        L = len(seq_ids)\n",
    "        if L > self.max_len:\n",
    "            seq_ids = seq_ids[:self.max_len]\n",
    "            L = self.max_len\n",
    "            \n",
    "        # Pad Inputs\n",
    "        pad_len = self.max_len - L\n",
    "        input_ids = torch.tensor(seq_ids + [4] * pad_len, dtype=torch.long)\n",
    "        mask = torch.tensor([1] * L + [0] * pad_len, dtype=torch.bool)\n",
    "        \n",
    "        # Get Targets\n",
    "        if self.mode == 'train':\n",
    "            coords = self.coords_map[target_id]\n",
    "            # coords is numpy array of shape (seq_len, 3)\n",
    "            \n",
    "            # Handle length mismatch (truncate or pad labels)\n",
    "            # Note: labels length might differ from sequence length if some residues are missing coords\n",
    "            # For this baseline, we assume 1-to-1 mapping or truncate to min\n",
    "            \n",
    "            c_len = len(coords)\n",
    "            if c_len > self.max_len:\n",
    "                 coords = coords[:self.max_len]\n",
    "            \n",
    "            # Convert to tensor\n",
    "            target_tensor = torch.zeros((self.max_len, 3), dtype=torch.float32)\n",
    "            target_tensor[:len(coords)] = torch.tensor(coords, dtype=torch.float32)\n",
    "            \n",
    "            return input_ids, target_tensor, mask\n",
    "            \n",
    "        return input_ids, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ae5ebc",
   "metadata": {},
   "source": [
    "## 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048f4645",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNAModel(nn.Module):\n",
    "    def __init__(self, vocab_size=5, embed_dim=128, nhead=4, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, 3)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # x: [B, L]\n",
    "        # mask: [B, L]\n",
    "        x_embed = self.embedding(x)\n",
    "        \n",
    "        # Transformer expects src_key_padding_mask where True means IGNORE (pad)\n",
    "        # Our mask is 1 for KEEP, 0 for PAD. So we invert it.\n",
    "        padding_mask = ~mask if mask is not None else None\n",
    "        \n",
    "        out = self.transformer(x_embed, src_key_padding_mask=padding_mask)\n",
    "        coords = self.fc(out)\n",
    "        return coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a42194",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    dataset = RNADataset(CONFIG['data_dir'], max_len=CONFIG['max_len'])\n",
    "    dataloader = DataLoader(dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "    \n",
    "    model = RNAModel().to(CONFIG['device'])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['lr'])\n",
    "    criterion = nn.MSELoss(reduction='none') # We want to mask loss\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        count = 0\n",
    "        \n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "        for input_ids, targets, mask in progress_bar:\n",
    "            input_ids = input_ids.to(CONFIG['device'])\n",
    "            targets = targets.to(CONFIG['device'])\n",
    "            mask = mask.to(CONFIG['device'])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            preds = model(input_ids, mask)\n",
    "            \n",
    "            # Compute masked loss\n",
    "            loss = criterion(preds, targets)\n",
    "            loss = (loss.mean(dim=-1) * mask).sum() / mask.sum()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "            progress_bar.set_postfix({'loss': total_loss/count})\n",
    "            \n",
    "        print(f\"Epoch {epoch+1} finished. Avg Loss: {total_loss/count:.4f}\")\n",
    "        \n",
    "    torch.save(model.state_dict(), 'model.pth')\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

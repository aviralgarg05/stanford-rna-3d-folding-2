{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford RNA 3D Folding - Training Notebook\n",
    "\n",
    "**Competition**: Stanford RNA 3D Folding Part 2\n",
    "\n",
    "**Goal**: Predict 5 diverse 3D structures (C1' coordinates) for each RNA sequence\n",
    "\n",
    "**Metric**: TM-score (best of 5 predictions averaged across targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Running on {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'data_dir': '../input/stanford-rna-3d-folding-2',\n",
    "    'max_len': 512,\n",
    "    'batch_size': 8,\n",
    "    'epochs': 20,\n",
    "    'lr': 3e-4,\n",
    "    'min_lr': 1e-6,\n",
    "    'weight_decay': 0.01,\n",
    "    'gradient_clip': 1.0,\n",
    "    'warmup_epochs': 2,\n",
    "    'vocab_size': 5,\n",
    "    'embed_dim': 256,\n",
    "    'nhead': 8,\n",
    "    'num_layers': 6,\n",
    "    'num_predictions': 5,\n",
    "    'dropout': 0.1,\n",
    "    'seed': 42,\n",
    "    'num_workers': 2,\n",
    "    'save_path': './model.pth',\n",
    "}\n",
    "\n",
    "print('Configuration:')\n",
    "for k, v in CONFIG.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print('Loading data...')\n",
    "\n",
    "train_seq = pd.read_csv(os.path.join(CONFIG['data_dir'], 'train_sequences.csv'))\n",
    "val_seq = pd.read_csv(os.path.join(CONFIG['data_dir'], 'validation_sequences.csv'))\n",
    "print(f'Train sequences: {len(train_seq)}')\n",
    "print(f'Validation sequences: {len(val_seq)}')\n",
    "\n",
    "train_labels = pd.read_csv(os.path.join(CONFIG['data_dir'], 'train_labels.csv'))\n",
    "val_labels = pd.read_csv(os.path.join(CONFIG['data_dir'], 'validation_labels.csv'))\n",
    "print(f'Train label rows: {len(train_labels)}')\n",
    "print(f'Validation label rows: {len(val_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess labels\n",
    "print('Preprocessing labels...')\n",
    "\n",
    "def preprocess_labels(labels_df):\n",
    "    coords_dict = {}\n",
    "    labels_df = labels_df.copy()\n",
    "    labels_df['target_id'] = labels_df['ID'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "    target_ids = labels_df['target_id'].unique()\n",
    "    print(f'Processing {len(target_ids)} targets...')\n",
    "    \n",
    "    for target_id in tqdm(target_ids, desc='Building coords'):\n",
    "        target_data = labels_df[labels_df['target_id'] == target_id].sort_values('resid')\n",
    "        if 'x_1' in target_data.columns:\n",
    "            x = target_data['x_1'].values\n",
    "            y = target_data['y_1'].values\n",
    "            z = target_data['z_1'].values\n",
    "        else:\n",
    "            continue\n",
    "        coords = np.stack([x, y, z], axis=1)\n",
    "        coords = np.nan_to_num(coords, nan=0.0)\n",
    "        coords_dict[target_id] = coords.astype(np.float32)\n",
    "    return coords_dict\n",
    "\n",
    "train_coords = preprocess_labels(train_labels)\n",
    "val_coords = preprocess_labels(val_labels)\n",
    "\n",
    "print(f'Train: {len(train_coords)}, Val: {len(val_coords)}')\n",
    "\n",
    "del train_labels, val_labels\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class RNADataset(Dataset):\n",
    "    def __init__(self, seq_df, coords_dict, max_len=512, augment=False):\n",
    "        self.seq_df = seq_df.reset_index(drop=True)\n",
    "        self.coords_dict = coords_dict\n",
    "        self.max_len = max_len\n",
    "        self.augment = augment\n",
    "        self.base2int = {'A': 0, 'C': 1, 'G': 2, 'U': 3, 'N': 4}\n",
    "        valid_ids = set(coords_dict.keys())\n",
    "        self.seq_df = self.seq_df[self.seq_df['target_id'].isin(valid_ids)].reset_index(drop=True)\n",
    "        print(f'Dataset: {len(self.seq_df)}')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.seq_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.seq_df.iloc[idx]\n",
    "        target_id = row['target_id']\n",
    "        sequence = row['sequence']\n",
    "        seq_ids = [self.base2int.get(c.upper(), 4) for c in sequence]\n",
    "        coords = self.coords_dict[target_id][:self.max_len].copy()\n",
    "        orig_len = min(len(seq_ids), len(coords), self.max_len)\n",
    "        \n",
    "        if self.augment and np.random.random() > 0.5:\n",
    "            angle = np.random.uniform(0, 2 * np.pi)\n",
    "            c, s = np.cos(angle), np.sin(angle)\n",
    "            R = np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]], dtype=np.float32)\n",
    "            coords = coords @ R.T\n",
    "        \n",
    "        if len(seq_ids) > self.max_len:\n",
    "            seq_ids = seq_ids[:self.max_len]\n",
    "        else:\n",
    "            seq_ids = seq_ids + [4] * (self.max_len - len(seq_ids))\n",
    "        \n",
    "        if len(coords) < self.max_len:\n",
    "            coords = np.pad(coords, ((0, self.max_len - len(coords)), (0, 0)))\n",
    "        else:\n",
    "            coords = coords[:self.max_len]\n",
    "        \n",
    "        mask = np.zeros(self.max_len, dtype=bool)\n",
    "        mask[:orig_len] = True\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(seq_ids, dtype=torch.long),\n",
    "            torch.tensor(coords, dtype=torch.float32),\n",
    "            torch.tensor(mask, dtype=torch.bool),\n",
    "            orig_len\n",
    "        )\n",
    "\n",
    "train_dataset = RNADataset(train_seq, train_coords, CONFIG['max_len'], True)\n",
    "val_dataset = RNADataset(val_seq, val_coords, CONFIG['max_len'], False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, CONFIG['batch_size'], shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, CONFIG['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f'Train batches: {len(train_loader)}, Val batches: {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class RNAStructurePredictor(nn.Module):\n",
    "    def __init__(self, vocab_size=5, embed_dim=256, nhead=8, num_layers=6, num_predictions=5, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=4)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, max_len)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nhead, dim_feedforward=embed_dim*4, dropout=dropout, batch_first=True, norm_first=True, activation='gelu')\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.heads = nn.ModuleList([nn.Sequential(nn.Linear(embed_dim, embed_dim), nn.GELU(), nn.Dropout(dropout), nn.Linear(embed_dim, embed_dim//2), nn.GELU(), nn.Linear(embed_dim//2, 3)) for _ in range(num_predictions)])\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.dropout(self.pos_encoder(self.embedding(x)))\n",
    "        x = self.transformer(x, src_key_padding_mask=~mask if mask is not None else None)\n",
    "        return torch.stack([h(x) for h in self.heads], dim=3)\n",
    "\n",
    "model = RNAStructurePredictor(CONFIG['vocab_size'], CONFIG['embed_dim'], CONFIG['nhead'], CONFIG['num_layers'], CONFIG['num_predictions'], CONFIG['dropout'], CONFIG['max_len']).to(device)\n",
    "print(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "class RNALoss(nn.Module):\n",
    "    def forward(self, pred, target, mask):\n",
    "        target_exp = target.unsqueeze(3).expand_as(pred)\n",
    "        mse = ((pred - target_exp) ** 2).sum(dim=2)\n",
    "        mse_masked = mse * mask.unsqueeze(2).float()\n",
    "        mse_per_pred = mse_masked.sum(dim=1) / mask.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "        best_loss = mse_per_pred.min(dim=1)[0].mean()\n",
    "        avg_loss = mse_per_pred.mean()\n",
    "        return 0.5 * best_loss + 0.5 * avg_loss, best_loss\n",
    "\n",
    "criterion = RNALoss()\n",
    "optimizer = AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n",
    "scheduler = CosineAnnealingLR(optimizer, CONFIG['epochs'] - CONFIG['warmup_epochs'], CONFIG['min_lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train_epoch(model, loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss, n = 0, 0\n",
    "    if epoch < CONFIG['warmup_epochs']:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = CONFIG['lr'] * (epoch + 1) / CONFIG['warmup_epochs']\n",
    "    for seq, coords, mask, _ in tqdm(loader, desc=f'Train {epoch+1}'):\n",
    "        seq, coords, mask = seq.to(device), coords.to(device), mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss, _ = criterion(model(seq, mask), coords, mask)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['gradient_clip'])\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        n += 1\n",
    "    return total_loss / n\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, n = 0, 0\n",
    "    for seq, coords, mask, _ in tqdm(loader, desc='Val'):\n",
    "        seq, coords, mask = seq.to(device), coords.to(device), mask.to(device)\n",
    "        loss, _ = criterion(model(seq, mask), coords, mask)\n",
    "        total_loss += loss.item()\n",
    "        n += 1\n",
    "    return total_loss / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "print('Starting training...')\n",
    "best_val = float('inf')\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, epoch)\n",
    "    val_loss = validate(model, val_loader, criterion)\n",
    "    if epoch >= CONFIG['warmup_epochs']:\n",
    "        scheduler.step()\n",
    "    print(f'Epoch {epoch+1}: train={train_loss:.4f}, val={val_loss:.4f}, lr={optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        torch.save({'model': model.state_dict(), 'config': CONFIG, 'val_loss': val_loss}, CONFIG['save_path'])\n",
    "        print(f'  Saved best model!')\n",
    "\n",
    "print(f'Training complete! Best val loss: {best_val:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify saved model\n",
    "print('Verifying model...')\n",
    "ckpt = torch.load(CONFIG['save_path'])\n",
    "model.load_state_dict(ckpt['model'])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for seq, coords, mask, lens in val_loader:\n",
    "        pred = model(seq.to(device), mask.to(device))[0].cpu().numpy()\n",
    "        tgt = coords[0].numpy()\n",
    "        l = lens[0].item()\n",
    "        rmsd = np.sqrt(((pred[:l,:,0] - tgt[:l])**2).mean())\n",
    "        print(f'Sample RMSD: {rmsd:.2f} A')\n",
    "        break\n",
    "print(f'Model size: {os.path.getsize(CONFIG[\"save_path\"])/1e6:.1f} MB')\n",
    "print('Done! Download model.pth for inference.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
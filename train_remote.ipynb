{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf6ece97",
   "metadata": {},
   "source": [
    "# Stanford RNA 3D Folding Competition - Training Notebook\n",
    "\n",
    "This notebook trains a model to predict 5 diverse 3D RNA structures per sequence.\n",
    "\n",
    "## Competition Requirements:\n",
    "- Predict 5 different structures per target (ensemble predictions)\n",
    "- Output format: x_1, y_1, z_1, ..., x_5, y_5, z_5 for each residue\n",
    "- Evaluation: TM-score (best of 5 predictions)\n",
    "- Coordinates: C1' atom positions in Angstroms\n",
    "- Runtime limit: 8 hours GPU for inference\n",
    "\n",
    "## 1. Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184cff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "CONFIG = {\n",
    "    'data_dir': '../input/stanford-rna-3d-folding-2',\n",
    "    'max_len': 512,  # Increased to handle longer sequences\n",
    "    'batch_size': 8,  # Reduced for larger sequences\n",
    "    'epochs': 15,\n",
    "    'lr': 5e-4,\n",
    "    'num_predictions': 5,  # Must predict 5 structures\n",
    "    'embed_dim': 256,\n",
    "    'nhead': 8,\n",
    "    'num_layers': 6,\n",
    "    'dropout': 0.1,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(CONFIG['seed'])\n",
    "\n",
    "print(f\"Running on {CONFIG['device']}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ad239f",
   "metadata": {},
   "source": [
    "## 2. Dataset Implementation\n",
    "\n",
    "Key features:\n",
    "- Handles variable sequence lengths (pad/truncate)\n",
    "- Loads multiple reference structures from train_labels.csv\n",
    "- Properly maps sequences to their 3D coordinates\n",
    "- Returns mask for valid positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b076cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNADataset(Dataset):\n",
    "    def __init__(self, data_path, max_len=512, mode='train'):\n",
    "        self.mode = mode\n",
    "        self.max_len = max_len\n",
    "        self.base2int = {'A': 0, 'C': 1, 'G': 2, 'U': 3, 'N': 4}  # N for unknown\n",
    "        \n",
    "        # Load sequences\n",
    "        print(f\"Loading {mode} sequences...\")\n",
    "        if mode == 'train':\n",
    "            self.seq_df = pd.read_csv(os.path.join(data_path, 'train_sequences.csv'))\n",
    "            lbl_df = pd.read_csv(os.path.join(data_path, 'train_labels.csv'))\n",
    "        elif mode == 'val':\n",
    "            self.seq_df = pd.read_csv(os.path.join(data_path, 'validation_sequences.csv'))\n",
    "            lbl_df = pd.read_csv(os.path.join(data_path, 'validation_labels.csv'))\n",
    "        else:  # test mode\n",
    "            self.seq_df = pd.read_csv(os.path.join(data_path, 'test_sequences.csv'))\n",
    "            lbl_df = None\n",
    "        \n",
    "        print(f\"Loaded {len(self.seq_df)} sequences\")\n",
    "        \n",
    "        if lbl_df is not None:\n",
    "            # Extract target_id from ID column (format: targetid_residuenumber)\n",
    "            lbl_df['target_id'] = lbl_df['ID'].apply(lambda x: x.rsplit('_', 1)[0])\n",
    "            \n",
    "            # Find all coordinate columns (x_1, y_1, z_1, x_2, y_2, z_2, ...)\n",
    "            coord_cols = [col for col in lbl_df.columns if col.startswith(('x_', 'y_', 'z_'))]\n",
    "            self.num_structures = len([col for col in coord_cols if col.startswith('x_')])\n",
    "            print(f\"Found {self.num_structures} reference structures in labels\")\n",
    "            \n",
    "            # Build coordinates dictionary: target_id -> numpy array of shape (num_residues, 3)\n",
    "            # Note: We only use the first structure for training\n",
    "            self.coords_map = {}\n",
    "            print(\"Building coordinates map...\")\n",
    "            unique_targets = lbl_df['target_id'].unique()\n",
    "            print(f\"Processing {len(unique_targets)} unique targets...\")\n",
    "            \n",
    "            for idx, target_id in enumerate(unique_targets):\n",
    "                if idx % 500 == 0:\n",
    "                    print(f\"  Processed {idx}/{len(unique_targets)} targets\")\n",
    "                    \n",
    "                target_data = lbl_df[lbl_df['target_id'] == target_id].sort_values('resid')\n",
    "                \n",
    "                # Extract coordinates for the first structure only\n",
    "                coords = target_data[['x_1', 'y_1', 'z_1']].values  # shape: (num_residues, 3)\n",
    "                coords = np.nan_to_num(coords, nan=0.0)\n",
    "                self.coords_map[target_id] = coords\n",
    "            \n",
    "            print(f\"Completed coordinates map for {len(self.coords_map)} targets\")\n",
    "            \n",
    "            # Filter sequences that have labels\n",
    "            self.seq_df = self.seq_df[self.seq_df['target_id'].isin(self.coords_map.keys())]\n",
    "            self.seq_df = self.seq_df.reset_index(drop=True)\n",
    "            print(f\"After filtering: {len(self.seq_df)} sequences with labels\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.seq_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.seq_df.iloc[idx]\n",
    "        target_id = row['target_id']\n",
    "        seq_str = row['sequence']\n",
    "        \n",
    "        # Convert sequence to integer IDs\n",
    "        seq_ids = [self.base2int.get(c, 4) for c in seq_str]  # 4 for unknown\n",
    "        orig_len = len(seq_ids)\n",
    "        \n",
    "        # Truncate or pad to max_len\n",
    "        if len(seq_ids) > self.max_len:\n",
    "            seq_ids = seq_ids[:self.max_len]\n",
    "            actual_len = self.max_len\n",
    "        else:\n",
    "            actual_len = len(seq_ids)\n",
    "            seq_ids = seq_ids + [4] * (self.max_len - actual_len)  # pad with unknown token\n",
    "        \n",
    "        input_ids = torch.tensor(seq_ids, dtype=torch.long)\n",
    "        mask = torch.zeros(self.max_len, dtype=torch.bool)\n",
    "        mask[:actual_len] = True\n",
    "        \n",
    "            # Get coordinates: shape (num_residues, 3)\n",
    "            # Get coordinates: shape (num_residues, 3, num_structures)\n",
    "            coords = self.coords_map[target_id]\n",
    "            \n",
    "            # Truncate or pad coordinates\n",
    "            if len(coords) > self.max_len:\n",
    "                coords = coords[:self.max_len]\n",
    "            # Create padded tensor: (max_len, 3)\n",
    "            target_tensor = torch.zeros((self.max_len, 3), dtype=torch.float32)\n",
    "            target_tensor = torch.zeros((self.max_len, 3, self.num_structures), dtype=torch.float32)\n",
    "            target_tensor[:len(coords)] = torch.tensor(coords, dtype=torch.float32)\n",
    "            # Replicate to match num_predictions for training\n",
    "            # Shape: (max_len, 3, num_predictions) - same target for all predictions\n",
    "            target_tensor = target_tensor.unsqueeze(-1).repeat(1, 1, 5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return input_ids, mask, target_id                    return input_ids, target_tensor, mask, target_id                    return input_ids, mask, target_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ae5ebc",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "**Key Design Decisions:**\n",
    "- **Transformer Encoder**: Captures long-range dependencies in RNA sequences\n",
    "- **Multi-head prediction**: Outputs 5 diverse structures using different prediction heads\n",
    "- **Positional Encoding**: Helps model understand residue positions\n",
    "- **Ensemble Learning**: Each head learns different structural conformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048f4645",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding for sequence position information\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class RNAStructurePredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Model to predict 5 diverse 3D structures for RNA sequences\n",
    "    \n",
    "    Architecture:\n",
    "    - Embedding layer for RNA nucleotides\n",
    "    - Positional encoding\n",
    "    - Transformer encoder for sequence understanding\n",
    "    - 5 separate prediction heads for diverse structures\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=5, embed_dim=256, nhead=8, num_layers=6, \n",
    "                 num_predictions=5, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_predictions = num_predictions\n",
    "        \n",
    "        # Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=4)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim, max_len=max_len)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True  # Pre-LN for better training stability\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Separate prediction heads for each of the 5 structures\n",
    "        # This encourages diversity in predictions\n",
    "        self.prediction_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(embed_dim, embed_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(embed_dim // 2, 3)  # Output (x, y, z) coordinates\n",
    "            )\n",
    "            for _ in range(num_predictions)\n",
    "        ])\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights for better training\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len) - RNA sequence IDs\n",
    "            mask: (batch, seq_len) - True for valid positions, False for padding\n",
    "        \n",
    "        Returns:\n",
    "            coords: (batch, seq_len, 3, num_predictions) - Predicted 3D coordinates\n",
    "        \"\"\"\n",
    "        # Embedding + positional encoding\n",
    "        x_embed = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        x_embed = self.pos_encoder(x_embed)\n",
    "        \n",
    "        # Transformer expects padding_mask where True = IGNORE\n",
    "        # Our mask is True = KEEP, so we invert it\n",
    "        padding_mask = ~mask if mask is not None else None\n",
    "        \n",
    "        # Encode sequence\n",
    "        encoded = self.transformer(x_embed, src_key_padding_mask=padding_mask)\n",
    "        # (batch, seq_len, embed_dim)\n",
    "        \n",
    "        # Generate predictions from each head\n",
    "        predictions = []\n",
    "        for head in self.prediction_heads:\n",
    "            coords = head(encoded)  # (batch, seq_len, 3)\n",
    "            predictions.append(coords)\n",
    "        \n",
    "        # Stack predictions: (batch, seq_len, 3, num_predictions)\n",
    "        predictions = torch.stack(predictions, dim=3)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a42194",
   "metadata": {},
   "source": [
    "## 4. Training Loop\n",
    "\n",
    "**Training Strategy:**\n",
    "- Train model to predict coordinates for all 5 structures simultaneously\n",
    "- Use MSE loss averaged across all prediction heads\n",
    "- Mask out padding positions\n",
    "- Save best model based on validation loss\n",
    "- Add gradient clipping for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, config):\n",
    "    \"\"\"Train the RNA structure prediction model\"\"\"\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['lr'],\n",
    "        weight_decay=0.01,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler - warmup + cosine decay\n",
    "    num_training_steps = len(train_loader) * config['epochs']\n",
    "    num_warmup_steps = num_training_steps // 10\n",
    "    \n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0.0, 0.5 * (1.0 + np.cos(np.pi * progress)))\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    \n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    print(\"\\\\nStarting training...\")\n",
    "    print(f\"Total training steps: {num_training_steps}\")\n",
    "    print(f\"Warmup steps: {num_warmup_steps}\")\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_count = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']}\")\n",
    "        for batch in progress_bar:\n",
    "            input_ids, targets, mask, _ = batch\n",
    "            input_ids = input_ids.to(config['device'])\n",
    "            targets = targets.to(config['device'])  # (batch, seq_len, 3, num_structures)\n",
    "            mask = mask.to(config['device'])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(input_ids, mask)  # (batch, seq_len, 3, num_predictions)\n",
    "            \n",
    "            # Compute loss for all predictions\n",
    "            # Average loss across all structure predictions\n",
    "            loss = criterion(predictions, targets)  # (batch, seq_len, 3, num_predictions)\n",
    "            \n",
    "            # Mask out padding and average\n",
    "            mask_expanded = mask.unsqueeze(-1).unsqueeze(-1)  # (batch, seq_len, 1, 1)\n",
    "            loss = (loss * mask_expanded).sum() / (mask.sum() * 3 * config['num_predictions'])\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_count += 1\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': train_loss / train_count,\n",
    "                'lr': scheduler.get_last_lr()[0]\n",
    "            })\n",
    "        \n",
    "        avg_train_loss = train_loss / train_count\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                input_ids, targets, mask, _ = batch\n",
    "                input_ids = input_ids.to(config['device'])\n",
    "                targets = targets.to(config['device'])\n",
    "                mask = mask.to(config['device'])\n",
    "                \n",
    "                predictions = model(input_ids, mask)\n",
    "                loss = criterion(predictions, targets)\n",
    "                \n",
    "                mask_expanded = mask.unsqueeze(-1).unsqueeze(-1)\n",
    "                loss = (loss * mask_expanded).sum() / (mask.sum() * 3 * config['num_predictions'])\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_count += 1\n",
    "        \n",
    "        avg_val_loss = val_loss / val_count\n",
    "        \n",
    "        print(f\"\\\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.6f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.6f}\")\n",
    "        print(f\"  Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': avg_val_loss,\n",
    "                'config': config\n",
    "            }, 'model.pth')\n",
    "            print(f\"  âœ“ New best model saved! (Val Loss: {avg_val_loss:.6f})\")\n",
    "    \n",
    "    print(f\"\\\\nTraining complete!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"RNA 3D Structure Prediction - Training\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"\\\\nLoading datasets...\")\n",
    "    train_dataset = RNADataset(\n",
    "        CONFIG['data_dir'],\n",
    "        max_len=CONFIG['max_len'],\n",
    "        mode='train'\n",
    "    )\n",
    "    val_dataset = RNADataset(\n",
    "        CONFIG['data_dir'],\n",
    "        max_len=CONFIG['max_len'],\n",
    "        mode='val'\n",
    "    )\n",
    "    \n",
    "    print(f\"Train samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    print(\"\\\\nInitializing model...\")\n",
    "    model = RNAStructurePredictor(\n",
    "        vocab_size=5,\n",
    "        embed_dim=CONFIG['embed_dim'],\n",
    "        nhead=CONFIG['nhead'],\n",
    "        num_layers=CONFIG['num_layers'],\n",
    "        num_predictions=CONFIG['num_predictions'],\n",
    "        dropout=CONFIG['dropout'],\n",
    "        max_len=CONFIG['max_len']\n",
    "    ).to(CONFIG['device'])\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Train model\n",
    "    model = train_model(model, train_loader, val_loader, CONFIG)\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"Training pipeline completed successfully!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
